---
title: "Do Scandals and Shocks Affect Elections?"
author: "Janet Hernandez"
date: "2022-10-24"
slug: "week7-shocks"
---

# Do Shocks Have a Significant Impact on Elections? #

This week, I will be doing the first blog extension. I will be replicating the Dobbs NYT example from section, but with the shock of recent inflation news. I predict that rising coverage on the state of the economy and inflation would have a negative affect on the President's party (Democrats) for generic ballot support. I am unsure whether or not this would also translate to an increase in support for Republicans but I am certain that I predict to see some hesitation towards Democrats due to economic inflation and market performances. I will be using the NYT developer API to source my data for news coverage. 

I will also try to modify and revise my existing predictive model from week 6. 

```{r , include=FALSE}
library(dotenv)
library(jsonlite)
library(tidyverse)
library(lubridate)
library(ggplot2)
library(plotly)
library(scales)
library(rjson)
library(jtools)
library(htmlwidgets)
library(reactable)
library(sqldf)
library(blogdown)
library(ggthemes)
library(plyr)

# load up hidden api key
article_api <- "3tPdNNFHIqqJySrXcFHjvgaFeCIac11M"
semantic_api <- Sys.getenv("SEMANTIC_API")
# 
# # set base url
base_url_art <- "http://api.nytimes.com/svc/search/v2/articlesearch.json?q="
base_url_sem <- "http://api.nytimes.com/svc/semantic/v2/concept/name"
# 
# # set parameters
term <- "inflation+biden"
facet_field <- "day_of_week"
facet <- "true"
fq <- "&fq=glocation='United States'"
begin_date <- "20220101"
end_date <- "20221022"


complete_url <-paste0(base_url_art,fq =term,"&facet_field=",facet_field,"&facet=",facet,"&begin_date=",begin_date,"&end_date=",end_date,"&api-key=",article_api,sep = "")
# import dataset to R

sus <- jsonlite::fromJSON(complete_url, flatten = TRUE)

# # view how many hits
sus$response$meta$hits


hits <- sus$response$meta$hits
cat("There were ",hits," hits for the search terms 'inflation' + 'biden' during 2022 to date",sep = "")
max_pages <- round((hits / 10) - 1)

# # store all pages in list
pages <- list()

#  trying again - WORKS!!!
Sys.sleep(1)
sus0 <-jsonlite::fromJSON(paste0(complete_url, "&page=0"), flatten = TRUE)
sus1 <- jsonlite::fromJSON(paste0(complete_url, "&page=1"), flatten = TRUE)
sus2 <- jsonlite::fromJSON(paste0(complete_url, "&page=2"), flatten = TRUE)
Sys.sleep(16)
sus3 <- jsonlite::fromJSON(paste0(complete_url, "&page=3"), flatten = TRUE)
sus4 <- jsonlite::fromJSON(paste0(complete_url, "&page=4"), flatten = TRUE)
Sys.sleep(16)
sus5 <- jsonlite::fromJSON(paste0(complete_url, "&page=5"), flatten = TRUE)
sus6 <- jsonlite::fromJSON(paste0(complete_url, "&page=6"), flatten = TRUE)
sus7 <- jsonlite::fromJSON(paste0(complete_url, "&page=7"), flatten = TRUE)
Sys.sleep(16)
sus8 <- jsonlite::fromJSON(paste0(complete_url, "&page=8"), flatten = TRUE)
sus9 <- jsonlite::fromJSON(paste0(complete_url, "&page=9"), flatten = TRUE)
Sys.sleep(12)
sus10 <- jsonlite::fromJSON(paste0(complete_url, "&page=10"), flatten = TRUE)
Sys.sleep(15)
sus11 <- jsonlite::fromJSON(paste0(complete_url, "&page=11"), flatten = TRUE)
sus12 <- jsonlite::fromJSON(paste0(complete_url, "&page=12"), flatten = TRUE)
Sys.sleep(12)
sus13 <- jsonlite::fromJSON(paste0(complete_url, "&page=13"), flatten = TRUE)
sus14 <- jsonlite::fromJSON(paste0(complete_url, "&page=14"), flatten = TRUE)
Sys.sleep(12)
sus15 <- jsonlite::fromJSON(paste0(complete_url, "&page=15"), flatten = TRUE)
Sys.sleep(5)




NYTSearch <- rbind_pages(list(sus0$response$docs, sus1$response$docs, sus2$response$docs,sus3$response$docs, sus4$response$docs, sus5$response$docs, sus6$response$docs, sus7$response$docs, sus8$response$docs, sus9$response$docs,sus10$response$docs, sus11$response$docs, sus12$response$docs, sus13$response$docs, sus14$response$docs, sus15$response$docs ))


# save df
saveRDS(NYTSearch, file = "inflation_2022.RDS")

# reload
mydata <- readRDS("inflation_2022.RDS")

```

## Background Literature on Shocks and Elections 

• Natural disaster: Shark attack (Achen and Bartels 2017), tornado
(Healy and Malhotra 2010) ⇝ decreased support for incumbents
• Sports (Healy, Mo, Malhotra 2010): college football team losing
their game ⇝ decreased support for incumbents
• Lottery (Bagues and Esteve-Volart 2016): Towns winning Spanish
Christmas lottery ⇝ increased support for incumbents
Global comparison (Powell & Whitten 1993): Punish only when
the gov’t is doing worse than other gov’ts
• Ex: the speed of recovery


```{r visualization, echo=FALSE}
# visualization by week
# extract raw date
mydata <- mydata %>% 
  mutate(publ_date = substr(pub_date, 1, 10))

# mutate week variable
mydata <- mydata %>% 
  mutate(week = strftime(publ_date, format = "%V"))
```

## New York Times Coverage on Biden and Inflation

Below is a plot of how the coverage of the terms 'inflation' + 'Biden' have changed from the period between January 2022 until October 15th, 2022. We can clearly see there is a huge spike in the months of August between weeks 31-33. I am looking at the first 15 pages of coverage from the NYT API. This equates to 160 articles out of a total of around 1000 written. 

In the next plot, I will compare how this coverage lines up with general trends in the general ballot support for Democrats and Republicans in the same time period. 

```{r, weekly plot, echo=FALSE}
# plot
mydata %>% 
  group_by(week) %>% 
  dplyr::summarize(count = n()) %>% 
  ggplot(aes(week, count, group = 1, color = count)) +
  geom_line() + theme_light()+ labs(y = "Article Count", x = "Week",
         title = "Weekly NYT Articles Mentioning 'Inflation' + 'Biden' in 2022",
         color = "")  

```


```{r, gen ballot setup, echo=FALSE, message=FALSE, warning=FALSE}
#now comparing this to generic ballot
X538_generic_ballot_averages_2018_2022 <- read.csv("~/Downloads/Section data/drive-download-20220922T150729Z-001/538_generic_ballot_averages_2018-2022.csv")

gb <- X538_generic_ballot_averages_2018_2022

# convert dat
gb <- gb %>%
  mutate(date_ = mdy(date)) %>%
  mutate(year = substr(date_, 1, 4)) %>%
  filter(year == 2022) %>%
  mutate(week = strftime(date_, format = "%V")) # Jan 1 looks weird 

#get avg by party and week
dem <- gb %>%
  filter(candidate == 'Democrats')
x <- plyr::ddply(dem, .(week), function(z) mean(z$pct_estimate))
x$candidate <- c('Democrats')
x$avg_dem <- x$V1
x <- x %>%
   select(-V1)
x$avg_dem <-  round(x$avg_dem , digits = 1)

rep <- gb %>%
   filter(candidate == 'Republicans')
y <- plyr::ddply(rep, .(week), function(z) mean(z$pct_estimate))
y$candidate <- c('Republicans')
y$avg_rep <- y$V1
y <- y %>%
  select(-V1)
y$avg_rep <-  round(y$avg_rep, digits = 1)
#
#put all data frames into list
df_list <- list(gb, x, y)
#
# #merge all data frames together
polls_df <- df_list %>% reduce(full_join, by=c("candidate", "week"))
#
# # remove NAs
polls_df[] <-  t(apply(polls_df, 1, function(x) c(x[!is.na(x)], x[is.na(x)])))
#
polls_df <- polls_df %>%
   select(-avg_rep)
#
polls_df$avg_support <- polls_df$avg_dem
#
polls_df <- polls_df %>%
  select(-avg_dem)
#
# # keep only unique dates
polls_df <- polls_df %>%
   distinct(cycle, week, date_, avg_support, candidate) %>%
   filter(week != 52)

# visualize polls
my_colors <- c("blue", "red")

```

## Peak Inflation Coverage Lines Up With Unexpected Drops in Support

```{r, plot for generic ballot, echo=FALSE, message=FALSE, warning=FALSE}
polls_df %>%
  group_by(candidate == 'Democrats') %>%
  mutate(date_ = as.Date(date_, format = '%Y-%m-%d')) %>%
  ggplot(aes(x = week, y = avg_support,
             colour = candidate)) +
  scale_color_manual(values = my_colors)+
   geom_line(aes(group=candidate), size = 0.3) + geom_point(size = 0.3) +
     #scale_x_date(date_labels = "%m, %Y") +
   ylab("Generic Ballot Support") + xlab("Week") +
      theme_light() + geom_segment(x=("31"), xend=("31"),y=0,yend=32, lty=2, color="black", alpha=0.3) +
      geom_segment(x=("33"), xend=("33"),y=0,yend=32, lty=2, color="black", alpha=0.3) + 
      annotate("text", x=("30"), y=27, label="Peak Inflation Coverage Window\n in NYT", size=4) 

```

Above is a graph that compares the peak weeks of coverage as evidenced in the plot above (Weeks 31-33) to the results of general ballot support pollings in the same time period. 

## Inflation and Biden News Punishing the Republicans More? 

An interesting observation of this graph in particular is how the dates line up perfectly with an overall drop in approval ratings for both Democrats and Republicans. However, the drop is much more significant for the non-incumbent party (Republicans) rather than President Biden's party. One possible explanation for the drop in both Democratic and Republican support could be that the coverage on inflation critiqued both sides, causing overall disillusion with the state of government and the economy. Interestingly enough, Republicans seem to be more punished than Democrats in this data. This may be explained or investigated further by creating sentiment scores for how the coverage was portraying Biden's response as either positive, negative, or neutral. Furthermore, the last thing to consider is that correlation does not equate to causation so there may be a possibility than other (possibly more important or popular) news coverage that I didn't consider in these weeks impacted Americans' support of the parties. 


## My Existing Model and Predictions

My current model uses several inputs including local level economic data such as state unemployment, inflationary levels that are standard across the states, incumbency status for each district, ad spend by candidate, and expert polling averages. 

## Why I Am Not Including Shocks Into My Model...

This week, I am choosing to not include data on shocks. My reasoning for this is that firstly, I am unsure of how exactly to quantify shock value as a variable of input into my model. Secondly, I believe that generic ballot polling averages do just as good of a job as accounting for shocks. The result of change in popular support is what we are most interested in and that is already accounted for. 

Therefore, this week I am choosing to keep my model intact with some minor adjustments to bettter improve predictive power such as including a confidence interval. 

```{r w7 setup, include = FALSE}
#Vote and seat share data from prev environments

voteseatshare_2020 <- read.csv("~/Desktop/Syllabi for Fall 22/election analytics/Gov1347 Data/house nationwide vote and seat share by party 1948-2020.csv")

house_party_vote_share_by_district_1948_2020 <- read_csv("~/Desktop/Syllabi for Fall 22/election analytics/Gov1347 Data/house party vote share by district 1948-2020.csv")

## ECONOMIC DATA ##

# Data on GDP/quarter
economy <- read.csv("~/Desktop/Gov1347 Data/Gov1347 Projects/Section data 2/GDP_quarterly.csv")

# Data on RDI/quarter
RDI_quarterly <- read.csv("~/Desktop/Gov1347 Data/Gov1347 Projects/Section data 2/RDI_quarterly.csv")

# Data on unemployment
unemployment_national_quarterly_final <- read.csv("~/Desktop/Gov1347 Data/Gov1347 Projects/Section data 2/unemployment_national_quarterly_final.csv")

# Data on vote/seat share
popvote_df <- read.csv("~/Desktop/Gov1347 Data/Gov1347 Projects/Section data 2/house_popvote_seats.csv")

#state unemployment data
unemployment_state_monthly <- read.csv("~/Desktop/Gov1347 Data/Gov1347 Projects/Section data 2/unemployment_state_monthly.csv")


#DATA FROM FEC.gov
spending_bydistrict_2018 <- read.csv("~/Desktop/Gov1347 Data/Gov1347 Projects/ElectionBlog/2018_spending_by_district.csv")

house_party_vote_share_by_district <- read_csv("~/Desktop/Syllabi for Fall 22/election analytics/Gov1347 Data/house party vote share by district 1948-2020.csv")%>%
filter(raceYear == 2018) %>%
select(State, raceYear, Area, RepCandidate, DemCandidate, RepVotesMajorPercent, DemVotesMajorPercent, st_fips, state_abb, CD, district_num, district_id, WinnerParty)

#Load data just for year 2018
house_vote_share_by_district_2018 <- read_csv("~/Desktop/Syllabi for Fall 22/election analytics/Gov1347 Data/house party vote share by district 1948-2020.csv") %>%
filter(raceYear == 2018) %>%
select(State, raceYear, Area, RepCandidate, DemCandidate, RepVotesMajorPercent, DemVotesMajorPercent, st_fips, state_abb, CD, district_num, district_id, WinnerParty)

# Change from at large states w only one district to a code-able suffix
house_vote_share_by_district_2018$CD[grep(pattern = "-AL", x = house_vote_share_by_district_2018$CD)] <- c("AK-01","DE-01","MT-01","ND-01","SD-01", "VT-01","WY-01")

#Cleaned 2018 data from Ethan
ratings_share_2018 <-read.csv("~/Desktop/2018_ratings.csv")

#Incumbent list
incumbentslist <- read.csv("~/Desktop/incumb_dist_1948-2020 (3).csv")

# GROUND GAME LAB SECTION NOTES ##

dist_pv_df <- read.csv("~/Desktop/incumb_dist_1948-2020 (3).csv")
# read in cvap
cvap_district <- read.csv("~/Downloads/drive-download-20221016T234313Z-001/cvap_district_2012-2020_clean.csv")
# mutate geoid for merging
cvap_district <- cvap_district %>%
rename(st_cd_fips = geoid)

# select relevant years from voting data
table(dist_pv_df$year)
# 2012 - from 2018
# 2014, 2016, 2018, 2020 - from 2020
dist_pv_df <- dist_pv_df %>%
filter(year == 2018)
table(dist_pv_df$st_cd_fips)

# merge
dist_pv_cvap <- dist_pv_df %>%
inner_join(cvap_district, by = c('st_cd_fips', 'year'))

# mutate turnout
dist_pv_cvap <- dist_pv_cvap %>%
mutate(totalvotes = RepVotes + DemVotes,
turnout = totalvotes/cvap)

# mutate votes percent for glm
dist_pv_cvap <- dist_pv_cvap %>%
mutate(DemVotesMajorPct = DemVotesMajorPercent/100,
RepVotesMajorPct = RepVotesMajorPercent/100)

# drop uncontested seats
dist_pv_cvap_closed <- dist_pv_cvap %>%
filter(!is.na(DemCandidate), !is.na(RepCandidate)) %>%
mutate(DemVotesMajorPct = DemVotesMajorPercent/100,
RepVotesMajorPct = RepVotesMajorPercent/100)


```

```{r map data setup, include=FALSE}
require(sf)

# load geographic data
get_congress_map <- function(cong=114) {
tmp_file <- tempfile()
tmp_dir <- tempdir()
zp <- sprintf("https://cdmaps.polisci.ucla.edu/shp/districts114.zip",cong)
download.file(zp, tmp_file)
unzip(zipfile = tmp_file, exdir = tmp_dir)
fpath <- paste(tmp_dir, sprintf("districtShapes/districts114.shp",cong), sep = "/")
st_read(fpath)
}

# load 114th congress

cd114 <- get_congress_map(114)

cd114$DISTRICT <- as.numeric(cd114$DISTRICT)

cd114 <- cd114 %>% left_join(house_party_vote_share_by_district, by= c("STATENAME"= "State", "DISTRICT" = "district_num"))


#cd114 <- cd114 %>% inner_join(local_model_data, by= c("STATENAME"= "State", "DISTRICT" = "district_num"))

districts_simp <- rmapshaper::ms_simplify(cd114, keep = 0.01)
```

```{r including turn out, include=FALSE}
#Merge datasets from last week for local model and plot districts to see how it influences.

#Possible way to approach this is to take all the data i have from previous years on voter turnout on midterm years only and then create a table w a prediction for 2022 and append that to my last week model.

# read in cvap
turnout_gen <- read.csv("~/Downloads/drive-download-20221016T234313Z-001/cvap_district_2012-2020_clean.csv")
# mutate geo id for merging
turnout_gen <- turnout_gen %>%
rename(st_cd_fips = geoid)

turnout_gen <- incumbentslist %>%
inner_join(cvap_district, by = c('st_cd_fips', 'year'))

turnout_gen <- turnout_gen %>%
mutate(totalvotes = RepVotes + DemVotes,
turnout = totalvotes/cvap)
# Filter out for only midterm years and non-presidential bc I think that might have an influence / skew data to more turnout than we should expect empirically.
turnout_gen <- turnout_gen %>%
filter(year == 2012 | year == 2014 | year == 2018)


turnout <- glm(turnout ~ year + state.x + president_party + cvap + winner_candidate_inc, data = turnout_gen)

summary(turnout)

turnoutpred_2022 <- data.frame()

turnoutpred <- as.data.frame(predict(turnout, new_data = turnoutpred_2022))

turnout_gen$prediction22 <- turnoutpred$`predict(turnout, new_data = turnoutpred_2022)`

#Added column for 2022 prediction now I can go ahead and map how well my model is prediction expected turnout v. actual turnout


#add margin column
turnout_gen <- turnout_gen %>%
mutate(margin = turnout - prediction22)

turnout_18 <- turnout_gen %>%
filter(year == 2018)

districts_simp <- districts_simp %>%
inner_join(turnout_18, by = 'district_id')
```


```{r local model data build, include=FALSE}
economy_q4 <- economy %>%
filter(quarter_yr == 4)

RDI_q4 <- RDI_quarterly %>%
filter(quarter_yr == 4)

unemployment_q4 <- unemployment_national_quarterly_final %>%
filter(quarter_yr == 4)

economy_model_data <- popvote_df %>%
inner_join(economy_q4, by = "year")

economy_model_data <-economy_model_data %>%
inner_join(RDI_q4, by = "year")
#Include District Level Economy Data
# Clean and update economy state level
unemployment_local <- unemployment_state_monthly %>%
filter(Year == 2018)

unemployment_local <- unemployment_local %>%
select('State.and.area', Year, Month, Unemployed_prct)

# Clean and include local RDI data for just 2018 and then join to economy data
RDI_local <- RDI_q4 %>%
filter(year == 2018)

# RDI and unemployment merged
unemployment_local <- unemployment_local %>%
inner_join(RDI_q4, by = c("Year" = "year"))
# Include GDP here too
economy_q4 <- economy_q4 %>%
select(year, GDP_growth_qt, GDP_growth_pct)

unemployment_local <- unemployment_local %>%
inner_join(economy_q4, by = c("Year" = "year"))

# Merge unemployment local to all state and district levels as equal. Merge by State.
unemployment_local <- rename(unemployment_local, State = 'State.and.area')


#Include District Level Polling Data (if possible)
#Using 2018 polling data to model our predictions
local_model_data_w6 <- house_vote_share_by_district_2018 %>%
inner_join(unemployment_local, by =  "State")


#Append the D and R Seats and national data to our local model for comparison in 2018.

natl_data_2018 <- economy_model_data %>%
select(year, R_seats, D_seats, R_majorvote_pct, D_majorvote_pct)

local_model_data_w6 <- local_model_data_w6 %>%
inner_join(natl_data_2018, by = c("Year" = "year"))

#Include District Incumbent Data

incumbentslist <- incumbentslist %>%
select(year, state, winner_party, district_id, winner_candidate_inc)

incumbentslist <- incumbentslist %>%
filter(year == 2018)

local_model_data_w6 <- local_model_data_w6 %>%
inner_join(incumbentslist, by = c("State" = "state","district_id" = "district_id"))

local_model_data_w6 <- local_model_data_w6 %>%
inner_join(ratings_share_2018, by = "CD")


## Ad spend

#Adding ad spending variable to our model

#Clean up ad data and add to local model then run lm

spending_bydistrict_2018 <- spending_bydistrict_2018 %>%
select(State, District, Candidate, Incumbent..Challenger.Open, Receipts)


#Need to add winning candidate name i think

local_model_data_w6 <- local_model_data_w6 %>%
mutate(winner_candidate = case_when( WinnerParty == "R" ~ local_model_data_w6$RepCandidate, WinnerParty == "D" ~ local_model_data_w6$DemCandidate))


local_model_data_w6 <- local_model_data_w6 %>%
inner_join(spending_bydistrict_2018, by = c("State" = "State", "district_num" ="District","winner_candidate_inc" = "Incumbent..Challenger.Open"))


local_model_data_w6$Receipts <- as.numeric(gsub("[\\$,]", "", local_model_data_w6$Receipts))

```

```{r}
#Take prediction for 2022 turnout and add to my existing model
w6_df <- data.frame()

w6_df <-districts_simp %>%
left_join(local_model_data_w6, by = "district_id", "CD")

# drop na
w6_cleaned <- w6_df %>%
filter(!is.na(avg_margin))

#replace missing polling values with toss up rating
w6_df$avg <- w6_df$avg %>%
replace_na(4.7)

w6_df$Unemployed_prct <- w6_df$Unemployed_prct %>%
replace_na(3.6)

w6_df$Receipts <- w6_df$Receipts %>%
replace_na(960454)

#Simplify shapefile
w6_df <- rmapshaper::ms_simplify(w6_df, keep = 0.01)
```

```{r w7 prediction}
w7_local_model <- glm(DemVotesMajorPercent.x ~ Unemployed_prct + winner_candidate_inc.x + Receipts + turnout + avg, data = w6_df)

summ(w7_local_model)

# add in Dem Vote prediction to w6_df and then filter for 2018, then append column to districts simp to plot
w6_df$DemVotePred22 <- predict(w6_local_model)

w7_simple <- w6_df %>%
select(State, Year, Month, cd, R_majorvote_pct, D_majorvote_pct, turnout, prediction22, DemVotesMajorPercent, DemVotePred22) %>%
filter(Year == 2018)

confint.default(w7_local_model)

```


## Final Prediction for Week 7 


## References 

Christopher H Achen and Larry M Bartels. Democracy for Realists: Why Elections Do Not Produce Responsive Government, volume 4. Princeton University Press, 2017. URL https://hollis.harvard.edu/primo-explore/fulldisplay?docid=TN_ cdi_askewsholts_vlebooks_9781400888740&context=PC&vid=HVD2&search_scope= DRAFT: everything&tab=everything&lang=en_US.

Marco Mendoza Avin ̃a and Semra Sevi. Did exposure to COVID-19 affect vote choice in the 2020 presidential election? Research & Politics, 8(3): 205316802110415, July 2021. ISSN 2053-1680, 2053-1680. doi: 10.1177/ 20531680211041505. URL https://hollis.harvard.edu/permalink/f/1mdq5o5/TN_cdi_doaj_primary_oai_doaj_org_article_f43f65041eb14d4f839740deb9063b43.

Andrew Healy, Neil Malhotra, et al. Random events, economic losses, and retrospective voting: Implications for democratic competence. Quarterly Journal of Political Science, 5 (2):193–208, 2010. URL https://hollis.harvard.edu/primo-explore/fulldisplaydocid=TN_cdi_crossref_primary_10_1561_100_00009057&context=PC&vid=HVD2&search_scope=everything&tab=everything&lang=en_US

Anthony Fowler and Andrew B Hall. Do Shark Attacks Influence Presidential Elections? Reassessing a Prominent Finding on Voter Competence. The Journal of politics, 80(4): 1423–1437, 2018. ISSN 1468-2508. URL https://hollis.harvard.edu/primo-explore/9fulldisplaydocid=TN_cdi_crossref_primary_10_1086_699244&context=PC&vid=HVD2&search_scope=everything&tab=everything&lang=en_US 

